{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37993d75-0969-4c48-9c1c-b316d34b6c81",
   "metadata": {},
   "source": [
    "## The one time Database creation cells are in RAW format while the ones that relate to simply retrieving results are runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502bf03d-3f74-433d-a309-3553d37bf8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from IPython.display import Markdown,display\n",
    "\n",
    "\n",
    "from getpass import getpass\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce000df5-59f4-47de-894a-afbf836aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.getcwd() \n",
    "file_path = os.path.join(current_folder, \"RAG Project Dataset\")\n",
    "db_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe53800-6f3d-4f72-841e-4282f4919c96",
   "metadata": {},
   "source": [
    "## Load the documents and split into Chunks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6871546d-be3a-4529-8103-0ecdff1aa191",
   "metadata": {},
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c0aa674-b6a7-4c69-a1d6-0e948cdc6e32",
   "metadata": {},
   "source": [
    "all_chunks = []\n",
    "for file_ in os.listdir(file_path):\n",
    "    loader = PyPDFLoader(file_path + \"\\\\\" + file_)\n",
    "    documents = loader.load()\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    file_chunks = [chunk.page_content for chunk in chunks]\n",
    "    all_chunks.extend(file_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125be4f4-021b-481e-a1bc-200eb7f34ec0",
   "metadata": {},
   "source": [
    "## Convert to Embeddings and Store in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f783810-1415-4163-9081-229688eab4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass()\n",
    "open_api_key = os.environ['OPENAI_API_KEY']\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\",temperature = 0, openai_api_key=open_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18adefd-abbb-49eb-930d-9f222e58b845",
   "metadata": {},
   "source": [
    "## Declaring the chromadb client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7875eb16-fbf3-40b3-a5ed-b7a00df10f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe0d78-477d-40a5-8eaa-1ff9ecfd356a",
   "metadata": {},
   "source": [
    "## Saving embeddings into ChromaDB - Run only for creating embedding database for the first time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "573f4d2e-763e-4d65-9d6b-9cefef867187",
   "metadata": {},
   "source": [
    "ids = (np.arange(1,len(all_chunks)+1)).tolist()\n",
    "ids = [str(x) for x in ids]\n",
    "embedded_chunks = [embedding_model.embed_query(chunk) for chunk in all_chunks]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd464e4a-8bb2-463c-949b-bd2bec01faed",
   "metadata": {},
   "source": [
    "collection = persistent_client.get_or_create_collection(\"knowledge_database\")\n",
    "collection.add(ids=ids, documents=all_chunks,\n",
    "               embeddings=embedded_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91471016-1a84-41a8-8ccc-f3a96161c576",
   "metadata": {},
   "source": [
    "## Loading the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb257f8-9864-4dbb-b53e-2b358b747fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"knowledge_database\",\n",
    "    embedding_function=embedding_model, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f33470-e6d1-4e96-8453-7b24518e9797",
   "metadata": {},
   "source": [
    "## Instantiating the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2846a856-9976-4b65-8b0c-f09111c6a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm, chain_type=\"stuff\", retriever=vector_store_from_client.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80dd18-03d8-499e-900b-a429c0355703",
   "metadata": {},
   "source": [
    "## Querying the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01528ca-9522-4c8d-a440-25c368c92aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"What are the main components of a RAG model and how do they interact?\"\n",
    "q2 = \"What are the two sub-layers in each encoder layer of the Transformer model?\"\n",
    "q3 = \"Explain how positional encoding is implemented in Transformers and why is it necessary?\"\n",
    "q4 = \"Describe the concept of multi-headed attention in the Transformers architecture.Why is it beneficial?\"\n",
    "q5 = \"What is few shot learning and how does GPT-3 implement it during inference?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c872a554-46f3-404d-b55d-43a484f405b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(\n",
    "    {\"query\": q5},\n",
    "    return_only_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28956c1e-accc-4f71-9dcc-94180c8ab49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Few-shot learning is a machine learning approach where a model is trained to perform a task with only a small number of examples or demonstrations. In the context of GPT-3, few-shot learning is implemented during inference by providing the model with a few examples of the task at hand as part of the input. This allows GPT-3 to adapt to the specific task without requiring any gradient updates or fine-tuning.\n",
       "\n",
       "During inference, GPT-3 takes in a prompt that includes the task description along with a few demonstrations of the desired output. The model then uses this context to generate responses that are relevant to the task, leveraging its pre-trained knowledge and the provided examples to perform effectively. This method allows GPT-3 to achieve strong performance on various NLP tasks by utilizing in-context learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(output['result']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
